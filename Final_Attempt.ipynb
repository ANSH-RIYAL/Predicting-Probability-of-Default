{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AkbvZ2eTyvN7"
   },
   "source": [
    "# Load packages + data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1669663284454,
     "user": {
      "displayName": "Ansh Riyal",
      "userId": "05981672545029206257"
     },
     "user_tz": 300
    },
    "id": "khN6U0CG38iU"
   },
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_cell_magic \n",
    "from IPython.display import HTML, display\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.stats as stats\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, accuracy_score, confusion_matrix, precision_score, recall_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "\n",
    "import pickle\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "DATA_PATH = './data/'\n",
    "DATA_FN   = \"train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 268,
     "status": "ok",
     "timestamp": 1669663284716,
     "user": {
      "displayName": "Ansh Riyal",
      "userId": "05981672545029206257"
     },
     "user_tz": 300
    },
    "id": "q6ZxLQcDqMo5",
    "outputId": "c67eaf13-a57d-4ac4-84c4-657fca03b31b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m3001FinalProject\u001b[m\u001b[m    README.md           \u001b[34mpred_prob_def\u001b[m\u001b[m\r\n",
      "Final_Attempt.ipynb \u001b[34mdata\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 9091,
     "status": "ok",
     "timestamp": 1669663293804,
     "user": {
      "displayName": "Ansh Riyal",
      "userId": "05981672545029206257"
     },
     "user_tz": 300
    },
    "id": "x8U57GWc56oU"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(DATA_PATH+DATA_FN)\n",
    "# print(data.shape)\n",
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yuGjeV54q7SE"
   },
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1669663293814,
     "user": {
      "displayName": "Ansh Riyal",
      "userId": "05981672545029206257"
     },
     "user_tz": 300
    },
    "id": "BcPKFKR4spJI"
   },
   "outputs": [],
   "source": [
    "# Aggregating terms\n",
    "def feature_extraction_modified(data,relevantAttributes):\n",
    "  current_liabilities = (data['debt_bank_st'] + data['debt_fin_st'])\n",
    "  current_assets_minus_inventory = data['asst_current'] # For now, we are considering no inventory\n",
    "  cash_mktbl_securities = current_assets_minus_inventory - data['AR']\n",
    "  working_capital = data['asst_current'] - current_liabilities # Also used as an indicator by itself\n",
    "  fixed_assets = data['asst_tang_fixed'] + data['asst_intang_fixed']\n",
    "\n",
    "  # Adding Liquidity ratios\n",
    "  # For other benchmarks, we can compare with the distribution of these ratios in the dataset (i.e. how many std deviations away from mean)\n",
    "\n",
    "  m = 1\n",
    "  current_ratio = (data['asst_current'] + m)/(data['asst_current'] + current_liabilities + m) \n",
    "\n",
    "  quick_ratio = (current_assets_minus_inventory + m)/(current_assets_minus_inventory + current_liabilities + m) \n",
    "\n",
    "  cash_ratio  = (cash_mktbl_securities + m)/(cash_mktbl_securities + current_liabilities + m)\n",
    "\n",
    "  # Operating Cash Flow Ratio\n",
    "  cfo_ratio = (data['cf_operations'] + m)/(data['cf_operations'] + current_liabilities + m) \n",
    "\n",
    "\n",
    "  # Adding Activity ratios\n",
    "\n",
    "  # Net revenue is net sales (according to the internet)\n",
    "\n",
    "  working_capital_turnover = (data['rev_operating'] + m)/(data['rev_operating'] + working_capital + m)\n",
    "\n",
    "  # Fixed assets are long term assets (according to the internet)\n",
    "\n",
    "  fixed_asset_turnover = (data['rev_operating'] + m)/(data['rev_operating'] + fixed_assets + m)\n",
    "\n",
    "  # # Adding extracted features to data:\n",
    "  # # Un comment all the below lines to add the extracted features into dataframe\n",
    "\n",
    "  # data['current_liabilities'] = current_liabilities\n",
    "  # data['current_assets_minus_inventory'] = current_assets_minus_inventory\n",
    "  # data['cash_mktbl_securities'] = cash_mktbl_securities\n",
    "  # data['working_capital'] = working_capital\n",
    "  # data['fixed_assets'] = fixed_assets\n",
    "  # data['current_ratio'] = current_ratio\n",
    "  data['quick_ratio'] = quick_ratio\n",
    "  # data['cash_ratio'] = cash_ratio\n",
    "  # data['cfo_ratio'] = cfo_ratio\n",
    "  # data['working_capital_turnover'] = working_capital_turnover\n",
    "  data['fixed_asset_turnover'] = fixed_asset_turnover\n",
    "\n",
    "  data = data.replace(np.nan,1)\n",
    "  # new_features = ['current_liabilities','current_assets_minus_inventory','cash_mktbl_securities','working_capital',\n",
    "  #                 'fixed_assets','current_ratio','quick_ratio','cash_ratio','cfo_ratio','working_capital_turnover',\n",
    "  #                 'fixed_asset_turnover']\n",
    "  new_features = ['quick_ratio','fixed_asset_turnover']\n",
    "  relevantAttributes = list(relevantAttributes)\n",
    "  relevantAttributes.extend(new_features)\n",
    "  relevantAttributes = np.asarray(relevantAttributes)\n",
    "  # print(relevantAttributes)\n",
    "\n",
    "  return data,relevantAttributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96rnXRst_cRi"
   },
   "source": [
    "# Harness for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0rp864Yqxkvt"
   },
   "source": [
    "## Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1669663293814,
     "user": {
      "displayName": "Ansh Riyal",
      "userId": "05981672545029206257"
     },
     "user_tz": 300
    },
    "id": "CyJyfPNR59mT"
   },
   "outputs": [],
   "source": [
    "def preprocessor(df, relevantAttributes, new=False):\n",
    "    df = df.replace(np.nan, 0) # replace all NaN with 0\n",
    "\n",
    "    # Replacing values with absolute values\n",
    "    for key in df.keys()[8:]:\n",
    "      if key not in ['AR','profit','roa','roe','wc_net','ebitda','prof_financing','prof_operating']:\n",
    "        df[key] = np.abs(df[key])\n",
    "\n",
    "    if new == False: # model building, need to create target variable\n",
    "      df['stmt_date'] = pd.to_datetime(df['stmt_date'], format='%Y-%m-%d') \n",
    "      df['def_date'] = pd.to_datetime(df['def_date'], format='%d/%m/%Y')  \n",
    "\n",
    "      df['default'] = np.where(df['fs_year']== df['def_date'].dt.year-1, 1, 0) # create default target \n",
    "\n",
    "      April_prior = df[df['def_date'].dt.month <= 4] # yes shift\n",
    "      April_after = df[(df['def_date'].dt.month > 4) | (pd.isnull(df['def_date'].dt.month))] # no shift\n",
    "      April_prior[['def_date','default']] = April_prior[April_prior['def_date'].dt.month <= 4].groupby('id')[['def_date','default']].shift(-1) # shift\n",
    "      df = pd.concat([April_prior, April_after]) # recombine\n",
    "\n",
    "      df = df.dropna(subset = ['default']) # drop rows due to nulls being created by shift \n",
    "\n",
    "    new_df = df\n",
    "\n",
    "    # load saved mean/std parameters\n",
    "    with open(DATA_PATH+'parameters.pkl', 'rb') as f:\n",
    "      parameters = pickle.load(f)\n",
    "\n",
    "    # outlier handling\n",
    "    for key in relevantAttributes:\n",
    "      zscores = np.asarray(new_df[key] - parameters[key]['mean'])/parameters[key]['std']\n",
    "      # print(np.sum(np.isnan(zscores)))\n",
    "      outliers = (zscores > 1) + (zscores < -1)\n",
    "      new_df[key] = new_df[key]*(~outliers) + new_df[key]*(outliers)/100\n",
    "\n",
    "\n",
    "    # Normalization\n",
    "    for key in relevantAttributes:\n",
    "      new_df[key] = (new_df[key] - parameters[key]['mean'])/parameters[key]['std']\n",
    "\n",
    "    # Extract features \n",
    "    new_df,relevantAttributes = feature_extraction_modified(new_df, relevantAttributes)\n",
    "    # print(relevantAttributes)\n",
    "\n",
    "    return(new_df, relevantAttributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8yl0DFLXtd2Z"
   },
   "source": [
    "## Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1669663293815,
     "user": {
      "displayName": "Ansh Riyal",
      "userId": "05981672545029206257"
     },
     "user_tz": 300
    },
    "id": "FZUXExCetcvs"
   },
   "outputs": [],
   "source": [
    "def estimator(df, relevantAttributes, fitting_algo, loss_fn = 'binary_crossentropy'):\n",
    "    if fitting_algo == neural_net_model:\n",
    "      model = fitting_algo(df,relevantAttributes, loss_fn)\n",
    "    else:\n",
    "      model = fitting_algo(df, relevantAttributes)\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o31Xi_YQT6yc"
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kg2QimC3A-86"
   },
   "source": [
    "### Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1669663293815,
     "user": {
      "displayName": "Ansh Riyal",
      "userId": "05981672545029206257"
     },
     "user_tz": 300
    },
    "id": "dKfIkL_iT4xn"
   },
   "outputs": [],
   "source": [
    "def logit_estimation_fn(df, relevantAttributes):\n",
    "    relevantAttributes = \" + \".join(relevantAttributes)\n",
    "    model_logit = smf.logit(formula=f'default ~ {relevantAttributes}', data=df).fit()\n",
    "    return(model_logit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJohExyFBAEC"
   },
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1669663293815,
     "user": {
      "displayName": "Ansh Riyal",
      "userId": "05981672545029206257"
     },
     "user_tz": 300
    },
    "id": "3W9sYmepT9Xn"
   },
   "outputs": [],
   "source": [
    "def random_forest(df, relevantAttributes):\n",
    "    y = df['default']\n",
    "    X = df[relevantAttributes]\n",
    "    clf = RandomForestClassifier(n_estimators=50, max_depth=10, min_samples_split=10, random_state=0).fit(X,y)\n",
    "    return(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVbUT9JtBCR3"
   },
   "source": [
    "### Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1669663293815,
     "user": {
      "displayName": "Ansh Riyal",
      "userId": "05981672545029206257"
     },
     "user_tz": 300
    },
    "id": "oiRMnGb2Q4Cy"
   },
   "outputs": [],
   "source": [
    "def neural_net_model(df,relevantAttributes, loss_fn = 'binary_crossentropy'):\n",
    "  Y = df['default']\n",
    "  X = df[relevantAttributes]\n",
    "\n",
    "  n_dims = len(relevantAttributes)\n",
    "\n",
    "  model = Sequential()\n",
    "  model.add(Input((n_dims)))\n",
    "  model.add(Dense(16, activation= 'relu'))\n",
    "  model.add(Dense(8, activation= 'relu'))\n",
    "  model.add(Dense(1, activation = 'sigmoid'))\n",
    "  model.compile(loss = loss_fn, optimizer = 'adam', metrics = [tf.keras.metrics.Recall(),tf.keras.metrics.Accuracy(name=\"accuracy\", dtype=None),tf.keras.metrics.FalseNegatives(),tf.keras.metrics.FalsePositives()])\n",
    "  \n",
    "  model.fit(X, Y, epochs=3)\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ngw4rr5iuGs7"
   },
   "source": [
    "## Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1669663293816,
     "user": {
      "displayName": "Ansh Riyal",
      "userId": "05981672545029206257"
     },
     "user_tz": 300
    },
    "id": "llcD5gwDuJhG"
   },
   "outputs": [],
   "source": [
    "def predictor(new_df, relevantAttributes, model):\n",
    "    predictions = model.predict(new_df[relevantAttributes])\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIXx2oTBtJC-"
   },
   "source": [
    "## Walk-forward harness for tuning and cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1669663293816,
     "user": {
      "displayName": "Ansh Riyal",
      "userId": "05981672545029206257"
     },
     "user_tz": 300
    },
    "id": "rKxe5TeLTIdQ"
   },
   "outputs": [],
   "source": [
    "def walk_forward_harness(df, relevantAttributes, preprocessor, estimator, predictor, model_type, \n",
    "                         start, date_col = 'fs_year', step_size = 1, loss_fn = 'binary_crossentropy'):     \n",
    "    predictions = []\n",
    "    model_list = []\n",
    "    actual = []\n",
    "    stats_list = {key: [] for key in ['auc','fpr','tpr','threshold']}\n",
    "    \n",
    "    max_year = df[date_col].max()\n",
    "    \n",
    "    tempAttributes = np.copy(relevantAttributes)\n",
    "\n",
    "    for count in np.arange(0,int((max_year - start)/step_size - 1)): \n",
    "        # print(count) \n",
    "        relevantAttributes = np.copy(tempAttributes)\n",
    "        \n",
    "        # preprocess train set \n",
    "        train = df[df[date_col] <= start+count]\n",
    "        train, _ = preprocessor(train, relevantAttributes) \n",
    "\n",
    "        # preprocess test set        \n",
    "        test = df[df[date_col] == start+count+step_size+1]\n",
    "        test, relevantAttributes = preprocessor(test, relevantAttributes)\n",
    "        \n",
    "        # fit model and predict \n",
    "        model = estimator(train, relevantAttributes, model_type, loss_fn = loss_fn)\n",
    "        pred_prob = predictor(test, relevantAttributes, model)\n",
    "        pred_prob = np.reshape(pred_prob, (test.shape[0]))\n",
    "\n",
    "        # calculate statistics\n",
    "        pred_df = pd.DataFrame({'actual': test['default'], 'pred_prob': pred_prob})\n",
    "        auc = roc_auc_score(pred_df['actual'], pred_df['pred_prob'])\n",
    "        fpr,tpr,threshold = metrics.roc_curve(pred_df['actual'], pred_df['pred_prob'])\n",
    " \n",
    "        model_list.append(model)\n",
    "        predictions.append(pred_prob)\n",
    "        actual.append(pred_df['actual'])\n",
    "        stats_list['auc'].append(auc)\n",
    "        stats_list['fpr'].append(fpr)\n",
    "        stats_list['tpr'].append(tpr)\n",
    "        stats_list['threshold'].append(threshold)\n",
    "    \n",
    "    return(predictions, model_list, stats_list, actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9h3ViKkdljRz"
   },
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1669663293816,
     "user": {
      "displayName": "Ansh Riyal",
      "userId": "05981672545029206257"
     },
     "user_tz": 300
    },
    "id": "1JF1DFDClnKt"
   },
   "outputs": [],
   "source": [
    "def validation(stats_list, actual, predictions):\n",
    "  # average AUC\n",
    "  print(f\"Average AUC: {np.mean(stats_list['auc'])}\")\n",
    "\n",
    "  # optimal threshold\n",
    "  optimal_idx = np.argmax(stats_list['tpr'][-1] - stats_list['fpr'][-1])\n",
    "  optimal_threshold = stats_list['threshold'][-1][optimal_idx]\n",
    "\n",
    "  # confusion matrix, precision, recall, accuracy\n",
    "  y_true = np.asarray(actual[-1])\n",
    "  y_preds = np.asarray(predictions[-1])\n",
    "  confusion = confusion_matrix(y_true,y_preds>optimal_threshold)\n",
    "  print(\"confusion matrix:\")\n",
    "  print(confusion)\n",
    "  print(\"precision: {}\".format(precision_score(y_true,y_preds>optimal_threshold)))\n",
    "  print(\"recall: {}\".format(recall_score(y_true,y_preds>optimal_threshold)))\n",
    "  print(\"accuracy: {}\".format(accuracy_score(y_true,y_preds>optimal_threshold)))\n",
    "\n",
    "  # ROC curves\n",
    "  for i in range(len(stats_list['fpr'])): \n",
    "    plt.plot(stats_list['fpr'][i],stats_list['tpr'][i])\n",
    "    plt.title('ROC Curve')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZuHsJsWA496"
   },
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFAj7mKFyGuL"
   },
   "source": [
    "## Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1669663293816,
     "user": {
      "displayName": "Ansh Riyal",
      "userId": "05981672545029206257"
     },
     "user_tz": 300
    },
    "id": "FwRcPPwRioqY"
   },
   "outputs": [],
   "source": [
    "# relevantAttributes = ['asst_tot','eqty_tot','profit','ebitda','roa']\n",
    "# predictions, model_list, stats_list, actual = walk_forward_harness(data, \n",
    "#                                                            relevantAttributes, \n",
    "#                                                            preprocessor, \n",
    "#                                                            estimator, \n",
    "#                                                            predictor, \n",
    "#                                                            logit_estimation_fn, \n",
    "#                                                            start=data['fs_year'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1669663293817,
     "user": {
      "displayName": "Ansh Riyal",
      "userId": "05981672545029206257"
     },
     "user_tz": 300
    },
    "id": "iYJ4rK9FwoAH"
   },
   "outputs": [],
   "source": [
    "# validation(stats_list, actual, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1669663293817,
     "user": {
      "displayName": "Ansh Riyal",
      "userId": "05981672545029206257"
     },
     "user_tz": 300
    },
    "id": "r3nQHlxVoAc_"
   },
   "outputs": [],
   "source": [
    "# for model in model_list:\n",
    "#   print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dNSQmXM2VFPB"
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1669663293817,
     "user": {
      "displayName": "Ansh Riyal",
      "userId": "05981672545029206257"
     },
     "user_tz": 300
    },
    "id": "lBRIMxB4VKP6"
   },
   "outputs": [],
   "source": [
    "# relevantAttributes = ['asst_tot','eqty_tot','profit','ebitda','roa']\n",
    "# predictions, model_list, stats_list, actual = walk_forward_harness(data, \n",
    "#                                                            relevantAttributes,\n",
    "#                                                            preprocessor, \n",
    "#                                                            estimator, \n",
    "#                                                            predictor, \n",
    "#                                                            random_forest, \n",
    "#                                                            start=data['fs_year'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1669663293817,
     "user": {
      "displayName": "Ansh Riyal",
      "userId": "05981672545029206257"
     },
     "user_tz": 300
    },
    "id": "mqBQ8Z-q0ueE"
   },
   "outputs": [],
   "source": [
    "# validation(stats_list, actual, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HU42FPEr9lro"
   },
   "source": [
    "## Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1669663293818,
     "user": {
      "displayName": "Ansh Riyal",
      "userId": "05981672545029206257"
     },
     "user_tz": 300
    },
    "id": "fEwUUKGaNX1K"
   },
   "outputs": [],
   "source": [
    "# relevantAttributes = ['asst_tot','eqty_tot','profit','ebitda','roa']\n",
    "# predictions, model_list, stats_list, actual = walk_forward_harness(data, \n",
    "#                                                            relevantAttributes, \n",
    "#                                                            preprocessor, \n",
    "#                                                            estimator, \n",
    "#                                                            predictor, \n",
    "#                                                            neural_net_model, \n",
    "#                                                            start=data['fs_year'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1669663293818,
     "user": {
      "displayName": "Ansh Riyal",
      "userId": "05981672545029206257"
     },
     "user_tz": 300
    },
    "id": "7tUr2XAqNX3R"
   },
   "outputs": [],
   "source": [
    "# validation(stats_list, actual, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8M2S3dbQ5ju"
   },
   "source": [
    "# Walk-forward harness to train on all data and pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1669663293818,
     "user": {
      "displayName": "Ansh Riyal",
      "userId": "05981672545029206257"
     },
     "user_tz": 300
    },
    "id": "OKlHYz0SQ5ju"
   },
   "outputs": [],
   "source": [
    "def train_walk_forward_harness(data, relevantAttributes, preprocessor, predictor, model_type):\n",
    "  train, relevantAttributes = preprocessor(data, relevantAttributes)\n",
    "  model = estimator(train, relevantAttributes, model_type)\n",
    "\n",
    "  with open(DATA_PATH+'model_final.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "  return model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 270287,
     "status": "ok",
     "timestamp": 1669663564072,
     "user": {
      "displayName": "Ansh Riyal",
      "userId": "05981672545029206257"
     },
     "user_tz": 300
    },
    "id": "pyS66XcjRRH-",
    "outputId": "bf0ee8de-7659-4156-8e37-518f0a3fcac3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "31836/31836 [==============================] - 11s 326us/step - loss: 0.0514 - recall: 1.8772e-04 - accuracy: 0.0000e+00 - false_negatives: 10652.0000 - false_positives: 193.0000\n",
      "Epoch 2/3\n",
      "31836/31836 [==============================] - 10s 321us/step - loss: 0.0486 - recall: 0.0048 - accuracy: 0.0000e+00 - false_negatives: 10603.0000 - false_positives: 67.0000\n",
      "Epoch 3/3\n",
      "31836/31836 [==============================] - 10s 322us/step - loss: 0.0484 - recall: 0.0072 - accuracy: 0.0000e+00 - false_negatives: 10577.0000 - false_positives: 85.0000\n"
     ]
    }
   ],
   "source": [
    "relevantAttributes = ['asst_tot','eqty_tot','profit','ebitda','roa']\n",
    "model = train_walk_forward_harness(data, relevantAttributes, preprocessor, predictor, neural_net_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTT_ECk9QrrP"
   },
   "source": [
    "# Final walk-forward harness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 98,
     "status": "ok",
     "timestamp": 1669663592201,
     "user": {
      "displayName": "Ansh Riyal",
      "userId": "05981672545029206257"
     },
     "user_tz": 300
    },
    "id": "qxBW5xfyQrrP"
   },
   "outputs": [],
   "source": [
    "def test_walk_forward_harness(model, test_df, relevantAttributes, preprocessor, predictor):     \n",
    "    test_df, relevantAttributes = preprocessor(test_df, relevantAttributes, new=True)\n",
    "\n",
    "#     with open(DATA_PATH+'model_final.pkl', 'rb') as f:\n",
    "#         model = pickle.load(f)\n",
    "    \n",
    "    pred_prob = predictor(test_df, relevantAttributes, model)\n",
    "    pred_prob = np.reshape(pred_prob, (test_df.shape[0]))\n",
    "    pred_prob[np.isnan(pred_prob)] = 0.5 # just in case\n",
    "\n",
    "    return pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 84452,
     "status": "ok",
     "timestamp": 1669663677630,
     "user": {
      "displayName": "Ansh Riyal",
      "userId": "05981672545029206257"
     },
     "user_tz": 300
    },
    "id": "NHMpz7GiQrrQ",
    "outputId": "a05da3aa-e194-40a5-8cdd-765b434d4c06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 369us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1000, 44), (1000,))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# holdout_sample = pd.read_csv('holdout_sample.csv')\n",
    "holdout_sample = pd.read_csv('./data/train_demo_subset.csv')\n",
    "relevantAttributes = ['asst_tot','eqty_tot','profit','ebitda','roa']\n",
    "pred_prob = test_walk_forward_harness(model, holdout_sample, relevantAttributes, preprocessor, predictor)\n",
    "holdout_sample.shape, pred_prob.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small example of what the data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ateco_sector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>13790282</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>27270248</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>1101350054</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>25150236</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>14510879</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  ateco_sector\n",
       "194    13790282            28\n",
       "177    27270248            32\n",
       "109  1101350054            41\n",
       "18     25150236            46\n",
       "155    14510879            45"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holdout_sample.iloc[np.random.randint(10,200,5), [1,5]] # Cannot show full data due to privacy issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unnamed: 0',\n",
       " 'id',\n",
       " 'stmt_date',\n",
       " 'HQ_city',\n",
       " 'legal_struct',\n",
       " 'ateco_sector',\n",
       " 'def_date',\n",
       " 'fs_year',\n",
       " 'asst_intang_fixed',\n",
       " 'asst_tang_fixed',\n",
       " 'asst_fixed_fin',\n",
       " 'asst_current',\n",
       " 'AR',\n",
       " 'cash_and_equiv',\n",
       " 'asst_tot',\n",
       " 'eqty_tot',\n",
       " 'eqty_corp_family_tot',\n",
       " 'liab_lt',\n",
       " 'liab_lt_emp',\n",
       " 'debt_bank_st',\n",
       " 'debt_bank_lt',\n",
       " 'debt_fin_st',\n",
       " 'debt_fin_lt',\n",
       " 'AP_st',\n",
       " 'AP_lt',\n",
       " 'debt_st',\n",
       " 'debt_lt',\n",
       " 'rev_operating',\n",
       " 'COGS',\n",
       " 'prof_operations',\n",
       " 'goodwill',\n",
       " 'inc_financing',\n",
       " 'exp_financing',\n",
       " 'prof_financing',\n",
       " 'inc_extraord',\n",
       " 'taxes',\n",
       " 'profit',\n",
       " 'days_rec',\n",
       " 'ebitda',\n",
       " 'roa',\n",
       " 'roe',\n",
       " 'wc_net',\n",
       " 'margin_fin',\n",
       " 'cf_operations']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(holdout_sample.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
